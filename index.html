<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/14/build-tf2-0-env/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/14/build-tf2-0-env/" class="post-title-link" itemprop="url">Tensorflow-GPU 2.0安装环境搭建</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-12-14 20:14:08 / 修改时间：20:16:07" itemprop="dateCreated datePublished" datetime="2021-12-14T20:14:08+08:00">2021-12-14</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>#virtualenv –python=python3.6 –system-site-packages tf2<br>#创建py3虚拟环境<br>/home/work/anaconda3/bin/virtualenv –python=/home/work/lib/anaconda3/bin/python3.6 tf2</p>
<p>#进入虚拟环境<br>source /home/work/pyenv/tf2/bin/activate</p>
<p>Tensorflow2.0安装环境<br>版本注意：<br>NVIDIA驱动程序需要410.x或更高版本<br>CUDA的版本需要10.0(推荐10.0.130,不可以是10.1)<br>同时cudnn版本号需要大于7.4.1(目前推荐7.6.0)</p>
<h1 id="N卡算力参考-要求不低于3-0"><a href="#N卡算力参考-要求不低于3-0" class="headerlink" title="N卡算力参考(要求不低于3.0)"></a>N卡算力参考(要求不低于3.0)</h1><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-gpus">https://developer.nvidia.com/cuda-gpus</a></p>
<h1 id="NVIDIA驱动下载地址"><a href="#NVIDIA驱动下载地址" class="headerlink" title="NVIDIA驱动下载地址"></a>NVIDIA驱动下载地址</h1><p><a target="_blank" rel="noopener" href="https://www.nvidia.com/Download/index.aspx?lang=cn">https://www.nvidia.com/Download/index.aspx?lang=cn</a></p>
<p>#NVIDIA驱动版本查看<br>nvidia-smi</p>
<p>#cuda 版本查看<br>nvcc -V  或者 cat /usr/local/cuda/version.txt</p>
<p>#cudnn 版本<br>cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</p>
<p>#通过conda创建py镜像<br>conda create -n conda-tf2 python=3.6</p>
<p>#设置清华镜像<br>conda config –add channels <a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</a><br>conda config –add channels <a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</a><br>conda config –set show_channel_urls yes</p>
<p>#查看conda虚拟环境并激活<br>conda info –envs<br>conda activate conda-tf2<br>conda list</p>
<p>#安装cudnn、cuda<br>conda install cudnn=7.6.0<br>conda install cudatoolkit=10.0.130</p>
<p>#CUDA版本与显卡驱动版本匹配查询<br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html</a></p>
<p>#下载对应版本的显卡驱动<br>wget <a target="_blank" rel="noopener" href="http://cn.download.nvidia.com/tesla/410.129/nvidia-diag-driver-local-repo-rhel7-410.129-1.0-1.x86_64.rpm">http://cn.download.nvidia.com/tesla/410.129/nvidia-diag-driver-local-repo-rhel7-410.129-1.0-1.x86_64.rpm</a></p>
<p>#安装显卡驱动<br>chmod +x NVIDIA-Linux-x86_64-410.129-diagnostic.run<br>sh NVIDIA-Linux-x86_64-410.129-diagnostic.run –kernel-source-path=/usr/src/kernels/3.10.0-514.16.1.el7.x86_64 -k $(uname -r)</p>
<p>查询TensorFlow与cuda版本对应：<br><a target="_blank" rel="noopener" href="https://tensorflow.google.cn/install/source">https://tensorflow.google.cn/install/source</a></p>
<p>#查看Tensorflow GPU是否可用<br>import tensorflow as tf<br>tf.test.is_gpu_available()</p>
<p>#训练时指定GPU<br>CUDA_VISIBLE_DEVICES=0,1,2,3 python3 xx.py</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/14/optimizer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/14/optimizer/" class="post-title-link" itemprop="url">optimizer</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-12-14 17:15:28 / 修改时间：19:17:47" itemprop="dateCreated datePublished" datetime="2021-12-14T17:15:28+08:00">2021-12-14</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>参考 <a target="_blank" rel="noopener" href="http://ruder.io/optimizing-gradient-descent/index.html">An overview of gradient descent optimization algorithms</a></p>
<h4 id="1-Batch-Gradient-Descent"><a href="#1-Batch-Gradient-Descent" class="headerlink" title="1. Batch Gradient Descent"></a>1. Batch Gradient Descent</h4><p><strong>梯度更新规则:</strong></p>
<p>BGD 采用整个训练集的数据来计算 cost function 对参数的梯度：<br>$$<br>\theta = \theta - \eta \cdot \nabla_\theta J( \theta)<br>$$<br>在一次更新中，批梯度下降需要对整个数据集计算梯度，这导致更新非常缓慢，而且可能存在内存不足的问题。也不适用于数据实时更新模型。</p>
<p><strong>优点：</strong></p>
<ul>
<li>目标函数若为凸函数，能够保证收敛到全局最优值；若为非凸函数，能够收敛到局部最优值。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>由于每轮迭代都需要在整个数据集上计算一次，所以批量梯度下降可能非常慢。</li>
<li>训练数较多时，需要较大内存。</li>
<li>批量梯度下降不允许在线更新模型，例如新增实例。</li>
</ul>
<p><strong>代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_epochs):</span><br><span class="line">  params_grad = evaluate_gradient(loss_function, data, params)</span><br><span class="line">  params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure>
<p>我们会事先定义一个迭代次数 epoch，首先计算梯度向量 params_grad，然后沿着梯度的方向更新参数 params，learning_rate 决定了我们每一步迈多大。<br>Batch gradient descent 对于凸函数可以收敛到全局极小值，对于非凸函数可以收敛到局部极小值。</p>
<h4 id="2-Stochastic-Gradient-Descent"><a href="#2-Stochastic-Gradient-Descent" class="headerlink" title="2. Stochastic Gradient Descent"></a>2. Stochastic Gradient Descent</h4><p><strong>梯度更新规则：</strong></p>
<p>$$<br>\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})<br>$$<br><strong>优点：</strong></p>
<ul>
<li>算法收敛速度快(在Batch Gradient Descent算法中, 每轮会计算很多相似样本的梯度, 这部分是冗余的)</li>
<li>可以在线更新。</li>
<li>有几率跳出一个比较差的局部最优而收敛到一个更好的局部最优甚至是全局最优。缺点：容易收敛到局部最优，并且容易被困在鞍点。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>容易收敛到局部最优，并且容易被困在鞍点。</li>
<li>SGD 因为更新比较频繁，会造成 cost function 有严重的震荡</li>
</ul>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_epochs):</span><br><span class="line">  np.random.shuffle(data)</span><br><span class="line">  <span class="keyword">for</span> batch <span class="keyword">in</span> get_batches(data, batch_size=<span class="number">50</span>):</span><br><span class="line">    params_grad = evaluate_gradient(loss_function, batch, params)</span><br><span class="line">    params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure>

<h4 id="3-Mini-batch-Gradient-Descent"><a href="#3-Mini-batch-Gradient-Descent" class="headerlink" title="3. Mini-batch Gradient Descent"></a>3. Mini-batch Gradient Descent</h4><p><strong>梯度更新规则：</strong></p>
<p>把数据分成若干个批，按批来更新参数，这样一批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。批的样本数与整个数据集相比小了很多，计算量也不是很大。是sgd与bgd的折中。<br>$$<br>\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})<br>$$</p>
<p>Mini-batch Gradient Descent在每轮迭代中仅仅计算一个mini-batch的梯度，不仅计算效率高，而且收敛较为稳定。该方法是目前深度学训练中的主流方法。</p>
<p><strong>代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nb_epochs):</span><br><span class="line">  np.random.shuffle(data)</span><br><span class="line">  <span class="keyword">for</span> batch <span class="keyword">in</span> get_batches(data, batch_size=<span class="number">50</span>):</span><br><span class="line">    params_grad = evaluate_gradient(loss_function, batch, params)</span><br><span class="line">    params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure>

<p>上述三个方法面临的<strong>主要挑战</strong>如下：</p>
<ul>
<li>选择适当的学习率α 较为困难。太小的学习率会导致收敛缓慢，而学习速度太块会造成较大波动，妨碍收敛，可能会跳过全局最优值。</li>
<li>目前可采用的方法是在训练过程中调整学习率大小，例如模拟退火算法：预先定义一个迭代次数m，每执行完m次训练便减小学习率，或者当损失函数的值低于一个阈值时减小学习率。然而迭代次数和阈值必须事先定义，因此无法适应数据集的特点。</li>
<li>上述方法中, 每个参数的 learning rate 都是相同的，这种做法是不合理的：如果训练数据是稀疏的，并且不同特征的出现频率差异较大，那么比较合理的做法是对于出现频率低的特征设置较大的学习速率，对于出现频率较大的特征数据设置较小的学习速率。</li>
</ul>
<h4 id="4-Momentum"><a href="#4-Momentum" class="headerlink" title="4. Momentum"></a>4. Momentum</h4><p>SGD方法的一个缺点是其更新方向完全依赖于当前batch计算出的梯度，因而十分不稳定。Momentum算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力。</p>
<p><strong>梯度更新规则：</strong></p>
<p>$$<br>v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta)<br>$$<br>$$<br>\theta = \theta - v_t<br>$$<br>Momentum算法会观察历史梯度v<sub>t−1</sub>，若当前梯度的方向与历史梯度一致（表明当前样本不太可能为异常点），则会增强这个方向的梯度，若当前梯度与历史梯方向不一致，则梯度会衰减。一种形象的解释是：我们把一个球推下山，球在下坡时积聚动量，在途中变得越来越快，γ可视为空气阻力，若球的方向发生变化，则动量会衰减。这里梯度需要有一个衰减值γ,推荐取0.9。</p>
<h5 id="此前我们都没有用到二阶动量。二阶动量的出现，才意味着“自适应学习率”优化算法时代的到来。SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到（想想大规模的embedding）。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些"><a href="#此前我们都没有用到二阶动量。二阶动量的出现，才意味着“自适应学习率”优化算法时代的到来。SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到（想想大规模的embedding）。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些" class="headerlink" title="此前我们都没有用到二阶动量。二阶动量的出现，才意味着“自适应学习率”优化算法时代的到来。SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到（想想大规模的embedding）。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些"></a>此前我们都没有用到二阶动量。二阶动量的出现，才意味着“自适应学习率”优化算法时代的到来。SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到（想想大规模的embedding）。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些</h5><h4 id="5-Adagrad"><a href="#5-Adagrad" class="headerlink" title="5. Adagrad"></a>5. Adagrad</h4><p>一种自适应学习率方法。上述方法中，对于每一个参数θ<sub>i</sub>的训练都使用了相同的学习率α。Adagrad算法能够在训练中自动的对learning rate进行调整，对于出现频率较低参数采用较大的α更新；相反，对于出现频率较高的参数采用较小的α更新。因此，Adagrad非常适合处理<strong>稀疏数据</strong>。</p>
<p><strong>梯度更新规则：</strong></p>
<p>设g<sub>t,i</sub>为第t轮时目标函数对第i个参数的偏导数（梯度），即<br>$$<br>g_{t, i} = \nabla_\theta J( \theta_{t, i} )<br>$$</p>
<p>普通的SGD在每一时刻t对参数θ<sub>i</sub>的更新公式为：</p>
<p>$$<br>\theta_{t+1, i} = \theta_{t, i} - \eta \cdot g_{t, i}<br>$$<br>Adagrad在每轮训练中对每个参数θ<sub>i</sub>的学习率进行更新，参数更新公式如下：<br>$$<br>\theta_{t+1, i} = \theta_{t, i} - \dfrac{\eta}{\sqrt{G_{t, ii} + \epsilon}} \cdot g_{t, i}<br>$$</p>
<p>其中<br>$$<br>G_{t} \in \mathbb{R}^{d \times d}<br>$$</p>
<p>为对角矩阵，每个对角线位置i,i为对应参数θ<sub>i</sub>第t轮梯度的平方和。ϵ是平滑项，用于避免分母为0，一般取值1e−8，学习率η一般取0.01。</p>
<p>Adagrad的缺点是在训练的中后期，分母上梯度平方的累加将会越来越大，从而梯度趋近于0，使得训练提前结束。</p>
<h4 id="6-Adadelta"><a href="#6-Adadelta" class="headerlink" title="6. Adadelta"></a>6. Adadelta</h4><p>Adadelta是对Aadgrad的扩展，目的是减少其单调递增的学习率。不同于Aadgrad采用累加过去所有梯度的平方和，Adadelta将分母的 G 换成了过去的梯度平方的衰减平均值。<br>$$<br>\Delta \theta_t = - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}<br>$$</p>
<p>Adadelta不是低效率地存储以前的平方梯度，而是递归地将梯度和定义为过去的梯度平方的衰减平均值。然后，在时间步骤t处的运行平均E[g<sup>2</sup>]<sub>t</sub>仅取决于先前的平均值E[g<sup>2</sup>]<sub>t-1</sub>和当前梯度g<sub>t</sub>(作为与动量项类似的分数γ, γ 一般设定为 0.9):<br>$$<br>E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g^2_t<br>$$</p>
<p>这个分母相当于梯度的均方根 root mean squared (RMS) ，所以可以用 RMS 简写：<br>$$<br>\Delta \theta_t = - \dfrac{\eta}{RMS[g]_{t}} g_t<br>$$<br>此外，还将学习率 η 换成了 RMS[Δθ]，这样的话，我们甚至都不需要提前设定学习率了：<br>$$<br>\Delta \theta_t = - \dfrac{RMS[\Delta \theta]_{t-1}}{RMS[g]_{t}} g_{t}<br>$$<br>$$<br>\theta_{t+1} = \theta_t + \Delta \theta_t<br>$$</p>
<h4 id="7-RMSprop"><a href="#7-RMSprop" class="headerlink" title="7. RMSprop"></a>7. RMSprop</h4><p>RMSprop 是 Geoff Hinton 提出的一种自适应学习率方法。</p>
<p>RMSprop 和 Adadelta 都是为了解决 Adagrad 学习率急剧下降问题的，</p>
<p><strong>梯度更新规则:</strong></p>
<p>RMSprop 与 Adadelta 的第一种形式相同：</p>
<p>$$<br>E[g^2]_t = 0.9 E[g^2]_{t-1} + 0.1 g^2_t<br>$$<br>$$<br>\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}<br>$$<br><strong>超参数设定值:</strong><br>Hinton 建议设定 γ 为 0.9, 学习率 η 为 0.001。</p>
<h4 id="8-Adam"><a href="#8-Adam" class="headerlink" title="8. Adam"></a>8. Adam</h4><p>Adam(Adaptive Moment Estimation)是另一种自适应学习率的方法。它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。</p>
<p>除了像 Adadelta 和 RMSprop 一样存储了过去梯度的平方 v<sub>t</sub> 的指数衰减平均值 ，也像 momentum 一样保持了过去梯度 m<sub>t</sub> 的指数衰减平均值我们分别计算过去和过去的平方梯度m<sub>t</sub>和v<sub>t</sub>的衰减平均值如下：</p>
<p>$$<br>m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t<br>$$<br>$$<br>v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2<br>$$<br>m<sub>t</sub>和v<sub>t</sub>分别为梯度的一阶矩和二阶矩</p>
<p>如果 m<sub>t</sub> 和 v<sub>t</sub> 被初始化为 0 向量，那它们就会向 0 偏置，所以做了偏差校正，<br>通过计算偏差校正后的 m<sub>t</sub> 和 v<sub>t</sub>来抵消这些偏差：</p>
<p>$$<br>\hat{m}_t = \dfrac{m_t}{1 - \beta^t_1}<br>$$<br>$$<br>\hat{v}_t = \dfrac{v_t}{1 - \beta^t_2}<br>$$<br><strong>梯度更新规则:</strong></p>
<p>$$<br>\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t<br>$$<br><strong>超参数设定值:</strong></p>
<p>建议 β1 ＝ 0.9，β2 ＝ 0.999，学习率 η 为 1e-3 or 5e-4 ，ϵ ＝ 10e−8</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/14/fisrt/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/14/fisrt/" class="post-title-link" itemprop="url">Rasa NLU</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-12-14 12:32:35 / 修改时间：12:34:41" itemprop="dateCreated datePublished" datetime="2021-12-14T12:32:35+08:00">2021-12-14</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Rasa-NLU"><a href="#Rasa-NLU" class="headerlink" title="Rasa NLU"></a>Rasa NLU</h2><img src=".\figure\f1.png" style="zoom:60%;" />

<h3 id="Tokenizers（分词）"><a href="#Tokenizers（分词）" class="headerlink" title="Tokenizers（分词）"></a>Tokenizers（分词）</h3><ul>
<li>JiebaTokenizer，针对中文分词</li>
<li>WhitespaceTokenizer，针对英文分词</li>
<li>ConveRTTokenizer，针对英文分词</li>
</ul>
<h3 id="Featurizer"><a href="#Featurizer" class="headerlink" title="Featurizer"></a>Featurizer</h3><p>Featurizer分为两种不同的类别:稀疏特征器和密集特征器。稀疏特征器由于只存储非零值，能够节省大量内存，能够在更大的数据集上进行训练，Featurizers可以返回两种不同的特征：序列特征与句子特征</p>
<ul>
<li>CountVectorsFeaturizer</li>
<li>LanguageModelFeaturizer</li>
</ul>
<p>以上两个Featurizer常用</p>
<h3 id="Language-Models"><a href="#Language-Models" class="headerlink" title="Language Models"></a>Language Models</h3><p>如果希望在pipline中使用预训练过的词向量，则需要加载预先训练过的模型</p>
<ul>
<li>MitieNLP</li>
<li>SpacyNLP</li>
<li>HFTransformersNLP</li>
</ul>
<h3 id="Intent-Classifier"><a href="#Intent-Classifier" class="headerlink" title="Intent Classifier"></a>Intent Classifier</h3><p>Intent Classifier将domain文件中定义的意图之一分配给传入的用户消息</p>
<ul>
<li>MitieIntentClassifier（需要MitieNLP and tokens）</li>
<li>SklearnIntentClassifier</li>
<li>KeywordIntentClassifier</li>
<li>DIETClassifier（同时可以做意图分类任务与entity提取任务）</li>
<li>FallbackClassifier（如果nlu意图分类分数不明确，则使用intent nlu回退对消息进行分类。confidence值设置为1 -最大confidence）</li>
</ul>
<h3 id="Entity-Extractors"><a href="#Entity-Extractors" class="headerlink" title="Entity Extractors"></a>Entity Extractors</h3><ul>
<li>MitieEntityExtractor（需要MitieNLP and tokens）</li>
</ul>
<h3 id="Combined-Intent-Classifiers-and-Entity-Extractors"><a href="#Combined-Intent-Classifiers-and-Entity-Extractors" class="headerlink" title="Combined Intent Classifiers and Entity Extractors"></a>Combined Intent Classifiers and Entity Extractors</h3><br/>

<ul>
<li>DIETClassifier（需要dense_features and/or sparse_features for user message and optionally the intent）</li>
</ul>
<img src=".\figure\f2.png" style="zoom:60%;" />

<br />

<img src=".\figure\f3.png" style="zoom:60%;" />

<br />

<img src=".\figure\f4.png" style="zoom:60%;" />
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
